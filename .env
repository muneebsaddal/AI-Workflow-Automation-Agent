# Ollama Configuration
# No API key needed - uses local Ollama instance!

# Ollama Server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_NUM_GPU=0
# Ollama Model to Use
# Options:
#   - mistral:latest (4.4GB) - Best accuracy, slower on CPU
#   - deepseek-coder:6.7b (3.8GB) - Good balance
#   - gemma3:270m (291MB) - RECOMMENDED for CPU, fastest
OLLAMA_MODEL=gemma3:270m

# GPU Configuration (0 = CPU only, prevents CUDA errors)
# Set to 0 if you get "CUDA error (status code: 500)"
OLLAMA_NUM_GPU=0

# Model Temperature (0 = deterministic, 1 = creative)
TEMPERATURE=0.1

# Request Timeout (increase for CPU inference)
OLLAMA_TIMEOUT=120

# Storage Files
STORAGE_FILE=tasks_db.json
LOGS_FILE=execution_logs.json